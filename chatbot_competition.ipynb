{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "import gensim\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import pandas as pd  \n",
    "import re\n",
    "nltk.download()\n",
    "import sys\n",
    "from bs4 import BeautifulSoup  \n",
    "import csv\n",
    "try:\n",
    "    from nltk import wordpunct_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "except ImportError:\n",
    "    print('[!] You need to install nltk (http://nltk.org/index.html)')\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calculate_languages_ratios(text):\n",
    "    languages_ratios = {}\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "    return languages_ratios\n",
    "\n",
    "def detect_language(text):\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "    return most_rated_language\n",
    "\n",
    "def text_language(text):\n",
    "    review_text = BeautifulSoup(text).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9éàèùêâôî]\", \" \", review_text) \n",
    "    #3. detect language\n",
    "    language = detect_language(letters_only)\n",
    "    return language\n",
    "\n",
    "def review_to_words(text):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(text).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9éàèùêâôî]\", \" \", review_text) \n",
    "    print(letters_only)\n",
    "    #3. detect language\n",
    "    language = detect_language(letters_only)\n",
    "    print(language)\n",
    "    stops = set(stopwords.words(language)) \n",
    "    # 4. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                                      \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    result = \" \".join( meaningful_words ) \n",
    "    return result \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 229 column 2 (char 7705)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-49ec7a566f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'300features_40minwords_10context'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conversation.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conversations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 229 column 2 (char 7705)"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('300features_40minwords_10context')\n",
    "file=open('conversation.json')\n",
    "data = json.load(file)\n",
    "cor=data[\"conversations\"]\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "\n",
    "for i in range(len(cor)):\n",
    "    for j in range(len(cor[i])):\n",
    "        if j<len(cor[i])-1:\n",
    "            x.append(cor[i][j])\n",
    "            y.append(cor[i][j+1])\n",
    "\n",
    "tok_x=[]\n",
    "tok_y=[]\n",
    "for i in range(len(x)):\n",
    "    tok_x.append(nltk.word_tokenize(x[i].lower()))\n",
    "    tok_y.append(nltk.word_tokenize(y[i].lower()))\n",
    "    \n",
    "    \n",
    "\n",
    "sentend=np.ones((300),dtype=np.float32) \n",
    "\n",
    "vec_x=[]\n",
    "for sent in tok_x:\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_x.append(sentvec)\n",
    "    \n",
    "vec_y=[]\n",
    "for sent in tok_y:\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_y.append(sentvec)           \n",
    "    \n",
    "    \n",
    "for tok_sent in vec_x:\n",
    "    tok_sent[20:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_x:\n",
    "    if len(tok_sent)<21:\n",
    "        for i in range(21-len(tok_sent)):\n",
    "            tok_sent.append(sentend)    \n",
    "            \n",
    "for tok_sent in vec_y:\n",
    "    tok_sent[20:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_y:\n",
    "    if len(tok_sent)<21:\n",
    "        for i in range(21-len(tok_sent)):\n",
    "            tok_sent.append(sentend)             \n",
    "            \n",
    "            \n",
    "with open('conversation.pickle','wb') as f:\n",
    "    pickle.dump([vec_x,vec_y],f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conversation.pickle', 'rb') as f:\n",
    "    vec_x,vec_y=pickle.load(f)    \n",
    "    \n",
    "vec_x=np.array(vec_x,dtype=np.float64)\n",
    "vec_y=np.array(vec_y,dtype=np.float64)    \n",
    "\n",
    "x_train,x_test, y_train,y_test = train_test_split(vec_x, vec_y, test_size=0.2, random_state=1)\n",
    "    \n",
    "model=Sequential()\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.compile(loss='cosine_proximity', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM1000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM1500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM2000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM2500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM3000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM3500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM4000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM4500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM5000.h5');          \n",
    "predictions=model.predict(x_test) \n",
    "mod = gensim.models.Word2Vec.load('300features_40minwords_10context');   \n",
    "[mod.most_similar([predictions[10][i]])[0] for i in range(21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=load_model('LSTM5000.h5')\n",
    "mod = gensim.models.Word2Vec.load('300features_40minwords_10context');\n",
    "print('Hello friend, I am danny, a chatbot here for you to have fun and help you in your daily life')\n",
    "print('I can speak with you in german, french, spanish or english')\n",
    "print('My creator trained me on some stupid datasets as he was too lazy ti look for something better')\n",
    "print('my knowledge of language comes from southpark I apologize if I hurt you :/')\n",
    "print('I am specialised in Hamburg sorry if you live elsewhere')\n",
    "while(True):\n",
    "    x=input(\"Enter the message:\");\n",
    "    lng = text_language(x)\n",
    "    if lng == 'german':\n",
    "        sent_tokenize(x)\n",
    "        blob = TextBlob(x)\n",
    "        x = blob.translate(to='en')\n",
    "        x = str(x)\n",
    "        print('You: ',x)\n",
    "    elif lng == 'french':\n",
    "        sent_tokenize(x)\n",
    "        blob = TextBlob(x)\n",
    "        x = blob.translate(to='en')\n",
    "        x = str(x)\n",
    "        print('You: ',x)\n",
    "    elif lng == 'spanish':\n",
    "        sent_tokenize(x)\n",
    "        blob = TextBlob(x)\n",
    "        x = blob.translate(to='en')\n",
    "        x = str(x)\n",
    "        print('You: ',x)\n",
    "    else:\n",
    "        output_text = x\n",
    "        print('You: ',x)\n",
    "    sentend=np.ones((300),dtype=np.float32) \n",
    "\n",
    "    sent=nltk.word_tokenize(x.lower())\n",
    "    sentvec = [mod[w] for w in sent if w in mod.wv.vocab]\n",
    "\n",
    "    sentvec[20:]=[]\n",
    "    sentvec.append(sentend)\n",
    "    if len(sentvec)<21:\n",
    "        for i in range(21-len(sentvec)):\n",
    "            sentvec.append(sentend) \n",
    "    sentvec=np.array([sentvec])\n",
    "    \n",
    "    predictions = model.predict(sentvec)\n",
    "    outputlist=[mod.most_similar([predictions[0][i]])[0][0] for i in range(21)]\n",
    "    answer=' '.join(outputlist)\n",
    "    \n",
    "    answer = answer.replace(\"tripe\", \"\")\n",
    "    answer = answer.replace(\"labor\", \"\")\n",
    "\n",
    "    if lng == 'german':\n",
    "        sent_tokenize(answer)\n",
    "        blob = TextBlob(answer)\n",
    "        answer = blob.translate(to='de')\n",
    "        print('Danny the Bot: ',answer)\n",
    "    elif lng == 'french':\n",
    "        sent_tokenize(answer)\n",
    "        blob = TextBlob(answer)\n",
    "        answer = blob.translate(to='fr')\n",
    "        print('Danny the Bot: ',answer)\n",
    "    elif lng == 'spanish':\n",
    "        sent_tokenize(answer)\n",
    "        blob = TextBlob(answer)\n",
    "        answer = blob.translate(to='es')\n",
    "        print('Danny the Bot: ',answer)\n",
    "    else:\n",
    "        answer = answer\n",
    "        print('Danny the Bot: ',answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
