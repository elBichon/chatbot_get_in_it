{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "import gensim\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import pandas as pd  \n",
    "import re\n",
    "nltk.download()\n",
    "import sys\n",
    "from bs4 import BeautifulSoup  \n",
    "import csv\n",
    "try:\n",
    "    from nltk import wordpunct_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "except ImportError:\n",
    "    print('[!] You need to install nltk (http://nltk.org/index.html)')\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "print(train.shape)\n",
    "print(train.columns.values)\n",
    "print(train[\"review\"][0])\n",
    "from bs4 import BeautifulSoup    \n",
    "example1 = BeautifulSoup(train[\"review\"][0])  \n",
    "print(train[\"review\"][0])\n",
    "print(example1.get_text())\n",
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",\" \", example1.get_text() )  # The text to search\n",
    "print(letters_only)\n",
    "lower_case = letters_only.lower()        \n",
    "words = lower_case.split()   \n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n",
    "clean_review = review_to_words( train[\"review\"][0] )\n",
    "print(clean_review)\n",
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )\n",
    "print(\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "clean_train_reviews = []\n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print( \"Review %d of %d\\n\" % ( i+1, num_reviews ))                                                                 \n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(train_data_features.shape)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)\n",
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "import nltk.data\n",
    "nltk.download()   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "print(sentences[1])\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calculate_languages_ratios(text):\n",
    "    languages_ratios = {}\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "    return languages_ratios\n",
    "\n",
    "def detect_language(text):\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "    return most_rated_language\n",
    "\n",
    "def text_language(text):\n",
    "    review_text = BeautifulSoup(text).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9éàèùêâôî]\", \" \", review_text) \n",
    "    #3. detect language\n",
    "    language = detect_language(letters_only)\n",
    "    return language\n",
    "\n",
    "def review_to_words(text):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(text).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9éàèùêâôî]\", \" \", review_text) \n",
    "    print(letters_only)\n",
    "    #3. detect language\n",
    "    language = detect_language(letters_only)\n",
    "    print(language)\n",
    "    stops = set(stopwords.words(language)) \n",
    "    # 4. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                                      \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    result = \" \".join( meaningful_words ) \n",
    "    return result \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('300features_40minwords_10context')\n",
    "file=open('conversation.json')\n",
    "data = json.load(file)\n",
    "cor=data[\"conversations\"]\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "\n",
    "for i in range(len(cor)):\n",
    "    for j in range(len(cor[i])):\n",
    "        if j<len(cor[i])-1:\n",
    "            x.append(cor[i][j])\n",
    "            y.append(cor[i][j+1])\n",
    "\n",
    "tok_x=[]\n",
    "tok_y=[]\n",
    "for i in range(len(x)):\n",
    "    tok_x.append(nltk.word_tokenize(x[i].lower()))\n",
    "    tok_y.append(nltk.word_tokenize(y[i].lower()))\n",
    "    \n",
    "    \n",
    "\n",
    "sentend=np.ones((300),dtype=np.float32) \n",
    "\n",
    "vec_x=[]\n",
    "for sent in tok_x:\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_x.append(sentvec)\n",
    "    \n",
    "vec_y=[]\n",
    "for sent in tok_y:\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_y.append(sentvec)           \n",
    "    \n",
    "    \n",
    "for tok_sent in vec_x:\n",
    "    tok_sent[20:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_x:\n",
    "    if len(tok_sent)<21:\n",
    "        for i in range(21-len(tok_sent)):\n",
    "            tok_sent.append(sentend)    \n",
    "            \n",
    "for tok_sent in vec_y:\n",
    "    tok_sent[20:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_y:\n",
    "    if len(tok_sent)<21:\n",
    "        for i in range(21-len(tok_sent)):\n",
    "            tok_sent.append(sentend)             \n",
    "            \n",
    "            \n",
    "with open('conversation.pickle','wb') as f:\n",
    "    pickle.dump([vec_x,vec_y],f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conversation.pickle', 'rb') as f:\n",
    "    vec_x,vec_y=pickle.load(f)    \n",
    "    \n",
    "vec_x=np.array(vec_x,dtype=np.float64)\n",
    "vec_y=np.array(vec_y,dtype=np.float64)    \n",
    "\n",
    "x_train,x_test, y_train,y_test = train_test_split(vec_x, vec_y, test_size=0.2, random_state=1)\n",
    "    \n",
    "model=Sequential()\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.compile(loss='cosine_proximity', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM1000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM1500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM2000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM2500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM3000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM3500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM4000.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM4500.h5');\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('LSTM5000.h5');          \n",
    "predictions=model.predict(x_test) \n",
    "mod = gensim.models.Word2Vec.load('300features_40minwords_10context');   \n",
    "[mod.most_similar([predictions[10][i]])[0] for i in range(21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=load_model('LSTM5000.h5')\n",
    "mod = gensim.models.Word2Vec.load('300features_40minwords_10context');\n",
    "print('Hello friend, I am danny, a chatbot here for you to have fun and help you in your daily life')\n",
    "print('I can speak with you in german, french, spanish or english')\n",
    "print('My creator trained me on some stupid datasets as he was too lazy to look for something better')\n",
    "print('my knowledge of language comes from southpark I apologize if I hurt you :/')\n",
    "print('I am specialised in Hamburg sorry if you live elsewhere')\n",
    "while(True):\n",
    "    x=input(\"Enter the message:\");\n",
    "    lng = text_language(x)\n",
    "    if lng == 'german':\n",
    "        sent_tokenize(x)\n",
    "        blob = TextBlob(x)\n",
    "        x = blob.translate(to='en')\n",
    "        x = str(x)\n",
    "        print('You: ',x)\n",
    "    elif lng == 'french':\n",
    "        sent_tokenize(x)\n",
    "        blob = TextBlob(x)\n",
    "        x = blob.translate(to='en')\n",
    "        x = str(x)\n",
    "        print('You: ',x)\n",
    "    elif lng == 'spanish':\n",
    "        sent_tokenize(x)\n",
    "        blob = TextBlob(x)\n",
    "        x = blob.translate(to='en')\n",
    "        x = str(x)\n",
    "        print('You: ',x)\n",
    "    else:\n",
    "        output_text = x\n",
    "        print('You: ',x)\n",
    "    sentend=np.ones((300),dtype=np.float32) \n",
    "\n",
    "    sent=nltk.word_tokenize(x.lower())\n",
    "    sentvec = [mod[w] for w in sent if w in mod.wv.vocab]\n",
    "\n",
    "    sentvec[20:]=[]\n",
    "    sentvec.append(sentend)\n",
    "    if len(sentvec)<21:\n",
    "        for i in range(21-len(sentvec)):\n",
    "            sentvec.append(sentend) \n",
    "    sentvec=np.array([sentvec])\n",
    "    \n",
    "    predictions = model.predict(sentvec)\n",
    "    outputlist=[mod.most_similar([predictions[0][i]])[0][0] for i in range(21)]\n",
    "    answer=' '.join(outputlist)\n",
    "    \n",
    "    answer = answer.replace(\"tripe\", \"\")\n",
    "    answer = answer.replace(\"labor\", \"\")\n",
    "\n",
    "    if lng == 'german':\n",
    "        sent_tokenize(answer)\n",
    "        blob = TextBlob(answer)\n",
    "        answer = blob.translate(to='de')\n",
    "        print('Danny the Bot: ',answer)\n",
    "    elif lng == 'french':\n",
    "        sent_tokenize(answer)\n",
    "        blob = TextBlob(answer)\n",
    "        answer = blob.translate(to='fr')\n",
    "        print('Danny the Bot: ',answer)\n",
    "    elif lng == 'spanish':\n",
    "        sent_tokenize(answer)\n",
    "        blob = TextBlob(answer)\n",
    "        answer = blob.translate(to='es')\n",
    "        print('Danny the Bot: ',answer)\n",
    "    else:\n",
    "        answer = answer\n",
    "        print('Danny the Bot: ',answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
